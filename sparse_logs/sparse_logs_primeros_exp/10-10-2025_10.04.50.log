2025-10-10 10:04:50.870 | INFO     | llmcompressor.metrics.logger:_create_default_logger:357 - Logging all LLM Compressor modifier-level logs to sparse_logs/10-10-2025_10.04.50.log
2025-10-10 10:04:50.871 | DEBUG    | llmcompressor.core.lifecycle:initialize:92 - Initializing compression lifecycle
2025-10-10 10:04:50.872 | INFO     | llmcompressor.recipe.recipe:create_instance:140 - Loading recipe from file /gaueko1/users/mmartin/ptq_exp/yaml/fp8_calib_dynamic_asym_recipe.yaml
2025-10-10 10:04:51.066 | DEBUG    | llmcompressor.core.lifecycle:initialize:105 - Initialized modifier: config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=False ended_=False
2025-10-10 10:04:51.066 | INFO     | llmcompressor.core.lifecycle:initialize:110 - Compression lifecycle initialized for 1 modifiers
2025-10-10 10:04:51.067 | INFO     | llmcompressor.pipelines.independent.pipeline:IndependentPipeline:43 - Inferred `SequentialPipeline` for `QuantizationModifier`
2025-10-10 10:04:56.599 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-10 10:04:56.600 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(kwargs, labels, logits, loss):
    if labels is not None:
        loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)
    return (loss,)
2025-10-10 10:04:56.600 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-10 10:04:56.605 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-10 10:04:56.605 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(input_ids, inputs_embeds):
    if (input_ids is None) ^ (inputs_embeds is not None):
        raise ValueError('You must specify exactly one of input_ids or inputs_embeds')
    return ()
2025-10-10 10:04:56.606 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-10 10:04:56.606 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-10 10:04:56.607 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_1(input_ids, inputs_embeds):
    if inputs_embeds is None:
        inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)
    return (inputs_embeds,)
2025-10-10 10:04:56.607 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-10 10:04:56.608 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-10 10:04:56.608 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_2(past_key_values, use_cache):
    if use_cache and past_key_values is None:
        past_key_values = DynamicCache(config=self.config)
    return (past_key_values,)
2025-10-10 10:04:56.609 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-10 10:04:56.609 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-10 10:04:56.610 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_3(cache_position, inputs_embeds, past_key_values, *, past_seen_tokens=None):
    if cache_position is None:
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        cache_position: torch.Tensor = torch.arange(past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device)
    return (cache_position, past_seen_tokens)
2025-10-10 10:04:56.610 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-10 10:04:56.610 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-10 10:04:56.610 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_4(cache_position, position_ids):
    if position_ids is None:
        position_ids = cache_position.unsqueeze(0)
    return (position_ids,)
2025-10-10 10:04:56.610 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-10 10:04:56.611 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-10 10:04:56.611 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_5(attention_mask, cache_position, inputs_embeds, past_key_values, position_ids):
    return create_causal_mask(config=self.config, input_embeds=inputs_embeds, attention_mask=attention_mask, cache_position=cache_position, past_key_values=past_key_values, position_ids=position_ids)
    return ()
2025-10-10 10:04:56.611 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-10 10:04:56.704 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.CALIBRATION_EPOCH_START
2025-10-10 10:04:56.704 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281d5490>
2025-10-10 10:04:56.706 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f9770625f10>
2025-10-10 10:04:56.706 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f9338193910>
2025-10-10 10:04:56.707 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6323ad0>
2025-10-10 10:04:56.708 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6330310>
2025-10-10 10:04:56.709 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6304d10>
2025-10-10 10:04:56.711 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f977062fad0>
2025-10-10 10:04:56.713 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6331e10>
2025-10-10 10:04:56.714 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f62f3d10>
2025-10-10 10:04:56.714 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97706167d0>
2025-10-10 10:04:56.714 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f9775544e90>
2025-10-10 10:04:56.714 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f62f1850>
2025-10-10 10:04:56.714 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f9778103ed0>
2025-10-10 10:04:56.715 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281cd2d0>
2025-10-10 10:04:56.715 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f62ea5d0>
2025-10-10 10:04:56.715 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f631cd10>
2025-10-10 10:04:56.715 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6306910>
2025-10-10 10:04:56.715 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f63216d0>
2025-10-10 10:04:56.715 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f633e990>
2025-10-10 10:04:56.715 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6348310>
2025-10-10 10:04:56.717 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f62f1590>
2025-10-10 10:04:56.720 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f63304d0>
2025-10-10 10:04:56.722 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281bd310>
2025-10-10 10:04:56.729 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281ce4d0>
2025-10-10 10:04:56.731 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281be2d0>
2025-10-10 10:04:56.732 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f63308d0>
2025-10-10 10:04:56.734 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f977061de90>
2025-10-10 10:04:56.735 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6348990>
2025-10-10 10:04:56.737 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281a6010>
2025-10-10 10:04:56.738 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6348a90>
2025-10-10 10:04:56.740 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6348b50>
2025-10-10 10:04:56.741 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281ce290>
2025-10-10 10:04:56.745 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f933814e210>
2025-10-10 10:04:56.746 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6331a10>
2025-10-10 10:04:56.748 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6348910>
2025-10-10 10:04:56.749 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f9770612cd0>
2025-10-10 10:04:56.751 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6331210>
2025-10-10 10:04:56.753 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6330e10>
2025-10-10 10:04:56.755 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93380fac90>
2025-10-10 10:04:56.757 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93381c0790>
2025-10-10 10:04:56.759 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f62f0810>
2025-10-10 10:04:56.761 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f63480d0>
2025-10-10 10:04:56.763 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281bce50>
2025-10-10 10:04:56.765 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f633fb50>
2025-10-10 10:04:56.767 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93380f9050>
2025-10-10 10:04:56.770 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f631ccd0>
2025-10-10 10:04:56.772 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281a7dd0>
2025-10-10 10:04:56.774 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f933814ccd0>
2025-10-10 10:04:56.776 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f633e210>
2025-10-10 10:04:56.778 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f63491d0>
2025-10-10 10:04:56.780 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f9770626990>
2025-10-10 10:04:56.783 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281cf190>
2025-10-10 10:04:56.785 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6320510>
2025-10-10 10:04:56.787 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f62f19d0>
2025-10-10 10:04:56.790 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281cf490>
2025-10-10 10:04:56.792 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281ce3d0>
2025-10-10 10:04:56.795 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6321a90>
2025-10-10 10:04:56.797 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6349a90>
2025-10-10 10:04:56.800 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f977556df90>
2025-10-10 10:04:56.802 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f633fad0>
2025-10-10 10:04:56.805 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6330d10>
2025-10-10 10:04:56.807 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281be390>
2025-10-10 10:04:56.810 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f933812fa90>
2025-10-10 10:04:56.813 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6331790>
2025-10-10 10:04:56.815 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f62f1e50>
2025-10-10 10:04:57.031 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281ce390>
2025-10-10 10:04:57.031 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f977061ccd0>
2025-10-10 10:04:57.032 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634a0d0>
2025-10-10 10:04:57.033 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634a1d0>
2025-10-10 10:04:57.034 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f62e9f90>
2025-10-10 10:04:57.036 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f62e9b10>
2025-10-10 10:04:57.036 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634a110>
2025-10-10 10:04:57.037 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f9770606450>
2025-10-10 10:04:57.037 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634a690>
2025-10-10 10:04:57.038 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f62e9650>
2025-10-10 10:04:57.039 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634a750>
2025-10-10 10:04:57.039 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93380f9390>
2025-10-10 10:04:57.040 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634a050>
2025-10-10 10:04:57.040 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97705e94d0>
2025-10-10 10:04:57.041 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281be110>
2025-10-10 10:04:57.041 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634aa50>
2025-10-10 10:04:57.042 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281d7090>
2025-10-10 10:04:57.043 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6323f90>
2025-10-10 10:04:57.043 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634ae50>
2025-10-10 10:04:57.044 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93380f8850>
2025-10-10 10:04:57.044 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634af90>
2025-10-10 10:04:57.045 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6323590>
2025-10-10 10:04:57.046 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f63330d0>
2025-10-10 10:04:57.046 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f933814e350>
2025-10-10 10:04:57.047 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634b250>
2025-10-10 10:04:57.048 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f631fb90>
2025-10-10 10:04:57.048 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634ab50>
2025-10-10 10:04:57.048 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634b2d0>
2025-10-10 10:04:57.049 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634afd0>
2025-10-10 10:04:57.049 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f933812f990>
2025-10-10 10:04:57.050 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281be1d0>
2025-10-10 10:04:57.051 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634b290>
2025-10-10 10:04:57.051 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634b550>
2025-10-10 10:04:57.052 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6320ed0>
2025-10-10 10:04:57.052 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281cd4d0>
2025-10-10 10:04:57.053 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f63315d0>
2025-10-10 10:04:57.054 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281d7250>
2025-10-10 10:04:57.054 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f9770610f10>
2025-10-10 10:04:57.055 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6305550>
2025-10-10 10:04:57.055 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281cd090>
2025-10-10 10:04:57.056 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f631ebd0>
2025-10-10 10:04:57.057 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281ce850>
2025-10-10 10:04:57.057 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6321550>
2025-10-10 10:04:57.058 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281bccd0>
2025-10-10 10:04:57.058 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634bb50>
2025-10-10 10:04:57.059 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f633e250>
2025-10-10 10:04:57.060 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6333890>
2025-10-10 10:04:57.060 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281cc190>
2025-10-10 10:04:57.061 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634bf90>
2025-10-10 10:04:57.061 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634bed0>
2025-10-10 10:04:57.062 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f633f910>
2025-10-10 10:04:57.063 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f631d590>
2025-10-10 10:04:57.063 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6322dd0>
2025-10-10 10:04:57.064 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281cdfd0>
2025-10-10 10:04:57.064 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f631dcd0>
2025-10-10 10:04:57.065 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f633e290>
2025-10-10 10:04:57.066 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281be210>
2025-10-10 10:04:57.066 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f62f1b50>
2025-10-10 10:04:57.067 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f631d350>
2025-10-10 10:04:57.067 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6305c10>
2025-10-10 10:04:57.068 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97705ed150>
2025-10-10 10:04:57.069 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f977062d950>
2025-10-10 10:04:57.069 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f933812fa10>
2025-10-10 10:04:57.070 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f63062d0>
2025-10-10 10:04:57.070 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f633dc90>
2025-10-10 10:04:57.071 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f63483d0>
2025-10-10 10:04:57.072 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281a4710>
2025-10-10 10:04:57.072 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281d4790>
2025-10-10 10:04:57.073 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f633cb50>
2025-10-10 10:04:57.073 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6304750>
2025-10-10 10:04:57.074 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f933812f9d0>
2025-10-10 10:04:57.075 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281cfa90>
2025-10-10 10:04:57.075 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281ce150>
2025-10-10 10:04:57.076 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635d010>
2025-10-10 10:04:57.076 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f633da10>
2025-10-10 10:04:57.077 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6305610>
2025-10-10 10:04:57.078 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93381c2890>
2025-10-10 10:04:57.078 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281bc690>
2025-10-10 10:04:57.079 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281a61d0>
2025-10-10 10:04:57.079 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93381c0850>
2025-10-10 10:04:57.080 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f62f3610>
2025-10-10 10:04:57.081 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281cd9d0>
2025-10-10 10:04:57.081 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281a4690>
2025-10-10 10:04:57.082 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f62f0990>
2025-10-10 10:04:57.082 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634b490>
2025-10-10 10:04:57.083 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281a4210>
2025-10-10 10:04:57.084 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f631e210>
2025-10-10 10:04:57.084 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93280ebf10>
2025-10-10 10:04:57.085 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f977061ded0>
2025-10-10 10:04:57.085 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6320190>
2025-10-10 10:04:57.086 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635c3d0>
2025-10-10 10:04:57.087 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6284a50>
2025-10-10 10:04:57.087 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635d910>
2025-10-10 10:04:57.088 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f631d190>
2025-10-10 10:04:57.088 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281d7ed0>
2025-10-10 10:04:57.089 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97705e9490>
2025-10-10 10:04:57.090 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281bff10>
2025-10-10 10:04:57.090 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f633ccd0>
2025-10-10 10:04:57.091 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f63484d0>
2025-10-10 10:04:57.091 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6349e50>
2025-10-10 10:04:57.092 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f977062d990>
2025-10-10 10:04:57.093 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281cfd10>
2025-10-10 10:04:57.093 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f977556f6d0>
2025-10-10 10:04:57.094 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6307110>
2025-10-10 10:04:57.094 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f9770610490>
2025-10-10 10:04:57.095 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281d7cd0>
2025-10-10 10:04:57.095 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281bca90>
2025-10-10 10:04:57.096 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f933812c8d0>
2025-10-10 10:04:57.097 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f633c350>
2025-10-10 10:04:57.097 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635ded0>
2025-10-10 10:04:57.098 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f62f1110>
2025-10-10 10:04:57.099 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f9770627810>
2025-10-10 10:04:57.099 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635e090>
2025-10-10 10:04:57.100 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6348550>
2025-10-10 10:04:57.100 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635e210>
2025-10-10 10:04:57.101 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635e310>
2025-10-10 10:04:57.102 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281ce350>
2025-10-10 10:04:57.102 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f9328197f50>
2025-10-10 10:04:57.103 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635e190>
2025-10-10 10:04:57.103 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f933814e510>
2025-10-10 10:04:57.103 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f633ff90>
2025-10-10 10:04:57.104 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f633fed0>
2025-10-10 10:04:57.105 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635e3d0>
2025-10-10 10:04:57.105 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f9770616b50>
2025-10-10 10:04:57.106 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635e450>
2025-10-10 10:04:57.106 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281ce410>
2025-10-10 10:04:57.107 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6307810>
2025-10-10 10:04:57.108 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f633fe50>
2025-10-10 10:04:57.108 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f933814e590>
2025-10-10 10:04:57.109 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f63485d0>
2025-10-10 10:04:57.109 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6307950>
2025-10-10 10:04:57.110 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6306dd0>
2025-10-10 10:04:57.111 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f9770624850>
2025-10-10 10:04:57.112 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f977061ed90>
2025-10-10 10:04:57.112 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281be750>
2025-10-10 10:04:57.113 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f9770627e90>
2025-10-10 10:04:57.113 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281d6710>
2025-10-10 10:04:57.114 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635f290>
2025-10-10 10:04:57.115 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f634a9d0>
2025-10-10 10:04:57.115 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635f510>
2025-10-10 10:04:57.116 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635f610>
2025-10-10 10:04:57.116 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93401bfe10>
2025-10-10 10:04:57.117 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281ce510>
2025-10-10 10:04:57.118 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f9770607510>
2025-10-10 10:04:57.118 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f631ef90>
2025-10-10 10:04:57.119 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6307a90>
2025-10-10 10:04:57.120 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635f650>
2025-10-10 10:04:57.120 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635fbd0>
2025-10-10 10:04:57.121 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635fa50>
2025-10-10 10:04:57.121 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281d6590>
2025-10-10 10:04:57.122 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635f450>
2025-10-10 10:04:57.123 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635fe90>
2025-10-10 10:04:57.123 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6284c10>
2025-10-10 10:04:57.124 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6348690>
2025-10-10 10:04:57.124 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281a6b10>
2025-10-10 10:04:57.125 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f6284850>
2025-10-10 10:04:57.126 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635f550>
2025-10-10 10:04:57.126 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f93281d65d0>
2025-10-10 10:04:57.127 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f97f635ff50>
2025-10-10 10:04:58.161 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='float', symmetric=False, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False
2025-10-10 10:05:00.550 | DEBUG    | llmcompressor.observers.base:record_observed_tokens:289 - The input tensor is expected to have two dimensions (batch_size * sequence_length, num_features). The input tensor has 3 dimensions.
