2025-10-09 07:27:19.132 | INFO     | llmcompressor.metrics.logger:_create_default_logger:357 - Logging all LLM Compressor modifier-level logs to sparse_logs/09-10-2025_07.27.19.log
2025-10-09 07:27:19.133 | DEBUG    | llmcompressor.core.lifecycle:initialize:92 - Initializing compression lifecycle
2025-10-09 07:27:19.134 | INFO     | llmcompressor.recipe.recipe:create_instance:140 - Loading recipe from file /gaueko1/users/mmartin/ptq_exp/yaml/awq_w4a16_asymetric_recipe.yaml
2025-10-09 07:27:19.545 | INFO     | llmcompressor.modifiers.awq.base:on_initialize:222 - No AWQModifier.mappings provided, inferring from model...
2025-10-09 07:27:19.564 | DEBUG    | llmcompressor.core.lifecycle:initialize:105 - Initialized modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=False ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:27:19.564 | INFO     | llmcompressor.core.lifecycle:initialize:110 - Compression lifecycle initialized for 1 modifiers
2025-10-09 07:27:19.564 | INFO     | llmcompressor.pipelines.independent.pipeline:IndependentPipeline:43 - Inferred `SequentialPipeline` for `AWQModifier`
2025-10-09 07:27:22.391 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-09 07:27:22.392 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(input_ids, inputs_embeds):
    if (input_ids is None) ^ (inputs_embeds is not None):
        raise ValueError('You must specify exactly one of input_ids or inputs_embeds')
    return ()
2025-10-09 07:27:22.392 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-09 07:27:22.392 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-09 07:27:22.392 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_1(input_ids, inputs_embeds):
    if inputs_embeds is None:
        inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)
    return (inputs_embeds,)
2025-10-09 07:27:22.392 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-09 07:27:22.392 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-09 07:27:22.392 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_2(past_key_values, use_cache):
    if use_cache and past_key_values is None:
        past_key_values = DynamicCache(config=self.config)
    return (past_key_values,)
2025-10-09 07:27:22.392 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-09 07:27:22.393 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-09 07:27:22.393 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_3(cache_position, inputs_embeds, past_key_values, *, past_seen_tokens=None):
    if cache_position is None:
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        cache_position: torch.Tensor = torch.arange(past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device)
    return (cache_position, past_seen_tokens)
2025-10-09 07:27:22.393 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-09 07:27:22.393 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-09 07:27:22.393 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_4(cache_position, position_ids):
    if position_ids is None:
        position_ids = cache_position.unsqueeze(0)
    return (position_ids,)
2025-10-09 07:27:22.393 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-09 07:27:22.394 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-09 07:27:22.394 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_5(attention_mask, cache_position, inputs_embeds, past_key_values, position_ids):
    return create_causal_mask(config=self.config, input_embeds=inputs_embeds, attention_mask=attention_mask, cache_position=cache_position, past_key_values=past_key_values, position_ids=position_ids)
    return ()
2025-10-09 07:27:22.394 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-09 07:27:22.397 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-09 07:27:22.397 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(kwargs, labels, logits, loss):
    if labels is not None:
        loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)
    return (loss,)
2025-10-09 07:27:22.397 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-09 07:27:22.489 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.CALIBRATION_EPOCH_START
2025-10-09 07:27:22.503 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f6408195450>
2025-10-09 07:27:22.503 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f6408118e50>
2025-10-09 07:27:22.503 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f64081c17d0>
2025-10-09 07:27:22.503 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c9619850>
2025-10-09 07:27:22.503 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ad510>
2025-10-09 07:27:22.503 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ad390>
2025-10-09 07:27:22.503 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ad8d0>
2025-10-09 07:27:22.503 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ada10>
2025-10-09 07:27:22.503 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96adc50>
2025-10-09 07:27:22.503 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96adb10>
2025-10-09 07:27:22.503 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96adf10>
2025-10-09 07:27:22.503 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ade10>
2025-10-09 07:27:22.504 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f64080dfbd0>
2025-10-09 07:27:22.504 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ae010>
2025-10-09 07:27:22.504 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ae3d0>
2025-10-09 07:27:22.504 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ae250>
2025-10-09 07:27:22.504 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ae610>
2025-10-09 07:27:22.504 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ae590>
2025-10-09 07:27:22.504 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ae990>
2025-10-09 07:27:22.504 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ae7d0>
2025-10-09 07:27:22.504 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96aec50>
2025-10-09 07:27:22.504 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96aed50>
2025-10-09 07:27:22.504 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96aef10>
2025-10-09 07:27:22.504 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96aebd0>
2025-10-09 07:27:22.504 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96af150>
2025-10-09 07:27:22.504 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96af1d0>
2025-10-09 07:27:22.505 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96af510>
2025-10-09 07:27:22.505 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96af210>
2025-10-09 07:27:22.505 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96af810>
2025-10-09 07:27:22.505 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96af750>
2025-10-09 07:27:22.505 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96afa90>
2025-10-09 07:27:22.505 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96afad0>
2025-10-09 07:27:22.505 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96afe10>
2025-10-09 07:27:22.505 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96afc90>
2025-10-09 07:27:22.505 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96aff50>
2025-10-09 07:27:22.505 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f64081b1c90>
2025-10-09 07:27:22.505 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bc190>
2025-10-09 07:27:22.505 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ad410>
2025-10-09 07:27:22.505 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bc4d0>
2025-10-09 07:27:22.505 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bc550>
2025-10-09 07:27:22.506 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bc910>
2025-10-09 07:27:22.506 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bc710>
2025-10-09 07:27:22.506 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bcd10>
2025-10-09 07:27:22.506 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bcc90>
2025-10-09 07:27:22.506 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bcf90>
2025-10-09 07:27:22.506 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96aef50>
2025-10-09 07:27:22.506 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bd050>
2025-10-09 07:27:22.506 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bd150>
2025-10-09 07:27:22.506 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bd4d0>
2025-10-09 07:27:22.506 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bd590>
2025-10-09 07:27:22.506 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bd750>
2025-10-09 07:27:22.506 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bd7d0>
2025-10-09 07:27:22.506 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bdb50>
2025-10-09 07:27:22.506 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bd990>
2025-10-09 07:27:22.507 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bdc90>
2025-10-09 07:27:22.507 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bded0>
2025-10-09 07:27:22.507 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bdf10>
2025-10-09 07:27:22.507 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bde10>
2025-10-09 07:27:22.507 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96be290>
2025-10-09 07:27:22.507 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f644df81690>
2025-10-09 07:27:22.507 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f64080dfa50>
2025-10-09 07:27:22.507 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96be510>
2025-10-09 07:27:22.507 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96be690>
2025-10-09 07:27:22.507 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96be710>
2025-10-09 07:27:22.507 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f6408125750>
2025-10-09 07:27:22.507 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96be910>
2025-10-09 07:27:22.507 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bed50>
2025-10-09 07:27:22.507 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f644dfb9650>
2025-10-09 07:27:22.508 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bed90>
2025-10-09 07:27:22.508 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bee90>
2025-10-09 07:27:22.508 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bec10>
2025-10-09 07:27:22.508 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bef50>
2025-10-09 07:27:22.508 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bf410>
2025-10-09 07:27:22.508 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f64081b1f90>
2025-10-09 07:27:22.508 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bf450>
2025-10-09 07:27:22.508 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ad150>
2025-10-09 07:27:22.508 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bf7d0>
2025-10-09 07:27:22.508 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bf610>
2025-10-09 07:27:22.508 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f6408126110>
2025-10-09 07:27:22.508 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bfa90>
2025-10-09 07:27:22.508 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bfcd0>
2025-10-09 07:27:22.508 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f644bfc0e90>
2025-10-09 07:27:22.509 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bfed0>
2025-10-09 07:27:22.509 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bf950>
2025-10-09 07:27:22.509 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cc0d0>
2025-10-09 07:27:22.509 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bcc10>
2025-10-09 07:27:22.509 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bff50>
2025-10-09 07:27:22.509 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cc350>
2025-10-09 07:27:22.509 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cc5d0>
2025-10-09 07:27:22.509 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cc450>
2025-10-09 07:27:22.509 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cc8d0>
2025-10-09 07:27:22.509 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cc590>
2025-10-09 07:27:22.509 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ccb50>
2025-10-09 07:27:22.509 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bcb50>
2025-10-09 07:27:22.509 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ccd50>
2025-10-09 07:27:22.509 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cccd0>
2025-10-09 07:27:22.510 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cca10>
2025-10-09 07:27:22.510 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ccf50>
2025-10-09 07:27:22.510 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cd290>
2025-10-09 07:27:22.510 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f6408101550>
2025-10-09 07:27:22.510 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bd090>
2025-10-09 07:27:22.510 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cd310>
2025-10-09 07:27:22.510 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cd590>
2025-10-09 07:27:22.510 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f6408101590>
2025-10-09 07:27:22.510 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cd8d0>
2025-10-09 07:27:22.510 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cd850>
2025-10-09 07:27:22.510 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cd4d0>
2025-10-09 07:27:22.510 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cdb50>
2025-10-09 07:27:22.510 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cdcd0>
2025-10-09 07:27:22.510 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96adad0>
2025-10-09 07:27:22.511 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bdf50>
2025-10-09 07:27:22.511 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f644dfc22d0>
2025-10-09 07:27:22.511 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cda90>
2025-10-09 07:27:22.511 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ce150>
2025-10-09 07:27:22.511 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ce390>
2025-10-09 07:27:22.511 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ce250>
2025-10-09 07:27:22.511 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ce310>
2025-10-09 07:27:22.511 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cdfd0>
2025-10-09 07:27:22.511 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ae290>
2025-10-09 07:27:22.511 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ce610>
2025-10-09 07:27:22.511 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cea10>
2025-10-09 07:27:22.511 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ce910>
2025-10-09 07:27:22.511 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bed10>
2025-10-09 07:27:22.511 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ced50>
2025-10-09 07:27:22.512 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ce9d0>
2025-10-09 07:27:22.512 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cef10>
2025-10-09 07:27:22.512 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cf090>
2025-10-09 07:27:22.512 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f6408126190>
2025-10-09 07:27:22.512 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cf050>
2025-10-09 07:27:22.512 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cf3d0>
2025-10-09 07:27:22.512 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cf5d0>
2025-10-09 07:27:22.512 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cf550>
2025-10-09 07:27:22.512 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cf790>
2025-10-09 07:27:22.512 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cf810>
2025-10-09 07:27:22.512 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cf4d0>
2025-10-09 07:27:22.512 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cf990>
2025-10-09 07:27:22.512 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cfbd0>
2025-10-09 07:27:22.512 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cf8d0>
2025-10-09 07:27:22.513 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cfed0>
2025-10-09 07:27:22.513 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cfd50>
2025-10-09 07:27:22.513 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96cffd0>
2025-10-09 07:27:22.513 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dc090>
2025-10-09 07:27:22.513 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dc310>
2025-10-09 07:27:22.513 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f6408126710>
2025-10-09 07:27:22.513 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f640c07f5d0>
2025-10-09 07:27:22.513 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dc590>
2025-10-09 07:27:22.513 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dc750>
2025-10-09 07:27:22.513 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dc2d0>
2025-10-09 07:27:22.513 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dc790>
2025-10-09 07:27:22.513 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dc990>
2025-10-09 07:27:22.513 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dc910>
2025-10-09 07:27:22.513 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dc9d0>
2025-10-09 07:27:22.514 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dce90>
2025-10-09 07:27:22.514 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dcf90>
2025-10-09 07:27:22.514 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dd150>
2025-10-09 07:27:22.514 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dd090>
2025-10-09 07:27:22.514 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dd4d0>
2025-10-09 07:27:22.514 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dce10>
2025-10-09 07:27:22.514 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dd510>
2025-10-09 07:27:22.514 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dd6d0>
2025-10-09 07:27:22.514 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f64081d4750>
2025-10-09 07:27:22.514 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bda50>
2025-10-09 07:27:22.514 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ddb90>
2025-10-09 07:27:22.514 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dda90>
2025-10-09 07:27:22.514 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ddc90>
2025-10-09 07:27:22.514 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bcd90>
2025-10-09 07:27:22.514 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ddf90>
2025-10-09 07:27:22.515 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ddcd0>
2025-10-09 07:27:22.515 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96de190>
2025-10-09 07:27:22.515 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96de2d0>
2025-10-09 07:27:22.515 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96de310>
2025-10-09 07:27:22.515 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96ae690>
2025-10-09 07:27:22.515 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f640c09fed0>
2025-10-09 07:27:22.515 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96bccd0>
2025-10-09 07:27:22.515 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96de990>
2025-10-09 07:27:22.515 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96de5d0>
2025-10-09 07:27:22.515 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dec10>
2025-10-09 07:27:22.515 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96deb10>
2025-10-09 07:27:22.515 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dee50>
2025-10-09 07:27:22.515 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f6408197650>
2025-10-09 07:27:22.515 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96df010>
2025-10-09 07:27:22.516 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96def90>
2025-10-09 07:27:22.516 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96df290>
2025-10-09 07:27:22.516 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96df210>
2025-10-09 07:27:22.516 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96df5d0>
2025-10-09 07:27:22.516 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96df6d0>
2025-10-09 07:27:22.516 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96df710>
2025-10-09 07:27:22.516 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96df9d0>
2025-10-09 07:27:22.516 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dfb90>
2025-10-09 07:27:22.516 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dfa10>
2025-10-09 07:27:22.516 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f6408126790>
2025-10-09 07:27:22.516 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True added <torch.utils.hooks.RemovableHandle object at 0x7f65c96dfb10>
2025-10-09 07:27:22.516 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:27:27.235 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:27:41.554 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:27:45.658 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:27:59.753 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:28:02.942 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:28:17.089 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:28:20.141 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:28:34.693 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:28:38.050 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:28:52.472 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:28:55.501 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:29:09.821 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:29:12.885 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:29:27.213 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:29:30.330 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:29:44.687 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:29:47.689 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:30:02.263 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:30:05.289 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:30:19.648 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:30:22.732 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:30:37.033 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:30:40.016 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:30:54.384 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:30:57.366 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:31:11.617 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:31:14.867 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:31:28.951 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:31:32.146 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:31:46.230 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:31:49.253 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:32:03.401 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:32:06.415 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:32:20.530 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:32:23.573 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:32:37.886 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:32:40.888 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:32:55.022 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:32:58.066 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:33:12.247 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:33:15.523 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:33:29.958 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:33:33.034 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:33:47.336 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:33:50.342 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:34:04.774 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:34:07.795 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:34:22.219 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:34:25.223 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:34:39.593 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:34:42.636 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:34:57.071 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:35:00.072 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:35:14.660 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:35:17.655 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:35:32.062 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:35:35.179 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:35:49.529 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:35:52.622 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:36:06.955 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:36:09.935 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:36:24.040 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:36:27.033 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:36:41.125 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:36:42.825 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2025-10-09 07:36:42.825 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:36:43.657 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.CALIBRATION_EPOCH_END
2025-10-09 07:36:47.458 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=True sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:36:47.461 | DEBUG    | llmcompressor.core.lifecycle:finalize:134 - Finalizing compression lifecycle
2025-10-09 07:36:47.461 | DEBUG    | llmcompressor.core.lifecycle:finalize:138 - Finalized modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W4A16_ASYM' kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=True started_=True ended_=True sequential_targets=None mappings=[AWQMapping(smooth_layer='re:.*input_layernorm$', balance_layers=['re:.*q_proj$', 're:.*k_proj$', 're:.*v_proj$']), AWQMapping(smooth_layer='re:.*v_proj$', balance_layers=['re:.*o_proj$']), AWQMapping(smooth_layer='re:.*post_attention_layernorm$', balance_layers=['re:.*gate_proj$', 're:.*up_proj$']), AWQMapping(smooth_layer='re:.*up_proj$', balance_layers=['re:.*down_proj$'])] offload_device=None duo_scaling=True
2025-10-09 07:36:47.461 | INFO     | llmcompressor.core.lifecycle:finalize:144 - Compression lifecycle finalized for 1 modifiers
2025-10-09 07:36:47.530 | INFO     | llmcompressor.transformers.compression.compressed_tensors_utils:get_model_compressor:193 - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.
2025-10-09 07:37:52.328 | DEBUG    | llmcompressor.transformers.utils.helpers:infer_recipe_from_model_path:105 - No recipe found in the model_path: /proiektuak/ikergaitu-data/azabala106/model_evaluation/trained_models/Latxa3.1_8b_lr1e-5
