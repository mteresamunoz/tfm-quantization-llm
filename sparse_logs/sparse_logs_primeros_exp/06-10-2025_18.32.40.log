2025-10-06 18:32:40.362 | INFO     | llmcompressor.metrics.logger:_create_default_logger:357 - Logging all LLM Compressor modifier-level logs to sparse_logs/06-10-2025_18.32.40.log
2025-10-06 18:32:40.373 | DEBUG    | llmcompressor.core.lifecycle:initialize:92 - Initializing compression lifecycle
2025-10-06 18:32:40.378 | INFO     | llmcompressor.recipe.recipe:create_instance:140 - Loading recipe from file /gaueko1/users/mmartin/ptq_exp/yaml/activ_quant_fp4_recipe.yaml
2025-10-06 18:32:40.910 | DEBUG    | llmcompressor.core.lifecycle:initialize:105 - Initialized modifier: config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=False ended_=False
2025-10-06 18:32:40.910 | INFO     | llmcompressor.core.lifecycle:initialize:110 - Compression lifecycle initialized for 1 modifiers
2025-10-06 18:32:40.914 | INFO     | llmcompressor.pipelines.independent.pipeline:IndependentPipeline:43 - Inferred `SequentialPipeline` for `QuantizationModifier`
2025-10-06 18:32:43.552 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-06 18:32:43.553 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(input_ids, inputs_embeds):
    if (input_ids is None) ^ (inputs_embeds is not None):
        raise ValueError('You must specify exactly one of input_ids or inputs_embeds')
    return ()
2025-10-06 18:32:43.553 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-06 18:32:43.553 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-06 18:32:43.553 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_1(input_ids, inputs_embeds):
    if inputs_embeds is None:
        inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)
    return (inputs_embeds,)
2025-10-06 18:32:43.553 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-06 18:32:43.553 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-06 18:32:43.553 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_2(past_key_values, use_cache):
    if use_cache and past_key_values is None:
        past_key_values = DynamicCache(config=self.config)
    return (past_key_values,)
2025-10-06 18:32:43.553 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-06 18:32:43.554 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-06 18:32:43.554 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_3(cache_position, inputs_embeds, past_key_values, *, past_seen_tokens=None):
    if cache_position is None:
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        cache_position: torch.Tensor = torch.arange(past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device)
    return (cache_position, past_seen_tokens)
2025-10-06 18:32:43.554 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-06 18:32:43.554 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-06 18:32:43.555 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_4(cache_position, position_ids):
    if position_ids is None:
        position_ids = cache_position.unsqueeze(0)
    return (position_ids,)
2025-10-06 18:32:43.555 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-06 18:32:43.555 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-06 18:32:43.556 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_5(attention_mask, cache_position, inputs_embeds, past_key_values, position_ids):
    return create_causal_mask(config=self.config, input_embeds=inputs_embeds, attention_mask=attention_mask, cache_position=cache_position, past_key_values=past_key_values, position_ids=position_ids)
    return ()
2025-10-06 18:32:43.556 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-06 18:32:43.559 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-06 18:32:43.559 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(kwargs, labels, logits, loss):
    if labels is not None:
        loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)
    return (loss,)
2025-10-06 18:32:43.559 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-06 18:32:43.651 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.CALIBRATION_EPOCH_START
2025-10-06 18:32:43.651 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8300bd0>
2025-10-06 18:32:43.652 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc805df10>
2025-10-06 18:32:43.652 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8301890>
2025-10-06 18:32:43.653 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f37f4170dd0>
2025-10-06 18:32:43.654 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7f91890>
2025-10-06 18:32:43.655 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f37dc0ff8d0>
2025-10-06 18:32:43.655 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27e5ecd0>
2025-10-06 18:32:43.656 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8309e10>
2025-10-06 18:32:43.656 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7faf7d0>
2025-10-06 18:32:43.657 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7fac690>
2025-10-06 18:32:43.658 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27d00710>
2025-10-06 18:32:43.658 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7fbbd90>
2025-10-06 18:32:43.658 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc805cf90>
2025-10-06 18:32:43.658 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc805d290>
2025-10-06 18:32:43.658 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27cf9c50>
2025-10-06 18:32:43.658 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8052690>
2025-10-06 18:32:43.658 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca831abd0>
2025-10-06 18:32:43.658 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca830ba10>
2025-10-06 18:32:43.659 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7fabad0>
2025-10-06 18:32:43.659 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7f92f10>
2025-10-06 18:32:43.659 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca831a910>
2025-10-06 18:32:43.659 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7fbbb90>
2025-10-06 18:32:43.659 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca82fa590>
2025-10-06 18:32:43.659 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7faf850>
2025-10-06 18:32:43.660 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca831b3d0>
2025-10-06 18:32:43.661 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca831b190>
2025-10-06 18:32:43.661 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca831b590>
2025-10-06 18:32:43.662 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7fba150>
2025-10-06 18:32:43.662 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8303bd0>
2025-10-06 18:32:43.663 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8036210>
2025-10-06 18:32:43.664 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7faa2d0>
2025-10-06 18:32:43.664 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27ce5390>
2025-10-06 18:32:43.665 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8053710>
2025-10-06 18:32:43.666 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca831af90>
2025-10-06 18:32:43.667 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27d0bc90>
2025-10-06 18:32:43.668 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca831b810>
2025-10-06 18:32:43.669 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27cfbb10>
2025-10-06 18:32:43.669 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7fab290>
2025-10-06 18:32:43.670 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca82fbd10>
2025-10-06 18:32:43.670 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8034450>
2025-10-06 18:32:43.672 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca831bfd0>
2025-10-06 18:32:43.672 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca83028d0>
2025-10-06 18:32:43.673 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27d0a7d0>
2025-10-06 18:32:43.673 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca831bed0>
2025-10-06 18:32:43.674 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc80356d0>
2025-10-06 18:32:43.675 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca83199d0>
2025-10-06 18:32:43.675 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8324290>
2025-10-06 18:32:43.676 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27ce73d0>
2025-10-06 18:32:43.677 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7f93050>
2025-10-06 18:32:43.678 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8303910>
2025-10-06 18:32:43.678 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca83243d0>
2025-10-06 18:32:43.679 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8324990>
2025-10-06 18:32:43.679 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8324a50>
2025-10-06 18:32:43.680 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8036110>
2025-10-06 18:32:43.681 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27d00ed0>
2025-10-06 18:32:43.681 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27cef410>
2025-10-06 18:32:43.683 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8324690>
2025-10-06 18:32:43.683 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8324c50>
2025-10-06 18:32:43.684 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8308190>
2025-10-06 18:32:43.684 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8035890>
2025-10-06 18:32:43.685 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8324dd0>
2025-10-06 18:32:43.686 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8036050>
2025-10-06 18:32:43.686 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca83250d0>
2025-10-06 18:32:43.687 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8324590>
2025-10-06 18:32:43.687 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8325450>
2025-10-06 18:32:43.688 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca83252d0>
2025-10-06 18:32:43.688 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8325590>
2025-10-06 18:32:43.688 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca83256d0>
2025-10-06 18:32:43.688 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7fa8d10>
2025-10-06 18:32:43.688 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8036090>
2025-10-06 18:32:43.689 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8325310>
2025-10-06 18:32:43.689 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8324d50>
2025-10-06 18:32:43.689 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7facdd0>
2025-10-06 18:32:43.694 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8325cd0>
2025-10-06 18:32:43.699 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8309650>
2025-10-06 18:32:43.703 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8325d90>
2025-10-06 18:32:43.708 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27d01b90>
2025-10-06 18:32:43.712 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8319790>
2025-10-06 18:32:43.717 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27cc6ed0>
2025-10-06 18:32:43.721 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7fae110>
2025-10-06 18:32:43.725 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca831ba50>
2025-10-06 18:32:43.726 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8326310>
2025-10-06 18:32:43.727 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8326250>
2025-10-06 18:32:43.727 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8037590>
2025-10-06 18:32:43.728 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8309b10>
2025-10-06 18:32:43.729 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8036410>
2025-10-06 18:32:43.729 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7fa9510>
2025-10-06 18:32:43.730 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27ceea90>
2025-10-06 18:32:43.731 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27cbf410>
2025-10-06 18:32:43.732 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc80363d0>
2025-10-06 18:32:43.732 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27ce5310>
2025-10-06 18:32:43.733 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca82f9990>
2025-10-06 18:32:43.733 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8326990>
2025-10-06 18:32:43.735 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8036350>
2025-10-06 18:32:43.735 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8324750>
2025-10-06 18:32:43.736 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8037d10>
2025-10-06 18:32:43.737 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8324ad0>
2025-10-06 18:32:43.737 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8326e50>
2025-10-06 18:32:43.737 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8037890>
2025-10-06 18:32:43.737 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7fa8f90>
2025-10-06 18:32:43.737 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8052990>
2025-10-06 18:32:43.737 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7fb8050>
2025-10-06 18:32:43.737 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8326210>
2025-10-06 18:32:43.737 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7fb81d0>
2025-10-06 18:32:43.737 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8326cd0>
2025-10-06 18:32:43.737 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca82f9590>
2025-10-06 18:32:43.738 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27cf28d0>
2025-10-06 18:32:43.738 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8319550>
2025-10-06 18:32:43.738 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27d017d0>
2025-10-06 18:32:43.738 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7faf4d0>
2025-10-06 18:32:43.738 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc80362d0>
2025-10-06 18:32:43.738 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27ce9210>
2025-10-06 18:32:43.739 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8326c50>
2025-10-06 18:32:43.739 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca83274d0>
2025-10-06 18:32:43.740 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8327990>
2025-10-06 18:32:43.741 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8037b90>
2025-10-06 18:32:43.741 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8036650>
2025-10-06 18:32:43.742 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8327c90>
2025-10-06 18:32:43.743 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8327c10>
2025-10-06 18:32:43.743 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca83277d0>
2025-10-06 18:32:43.744 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8327a50>
2025-10-06 18:32:43.744 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8327e10>
2025-10-06 18:32:43.745 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8327f90>
2025-10-06 18:32:43.746 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8327fd0>
2025-10-06 18:32:43.746 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7fd8bd0>
2025-10-06 18:32:43.746 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7fd91d0>
2025-10-06 18:32:43.746 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8327890>
2025-10-06 18:32:43.746 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27cbec50>
2025-10-06 18:32:43.746 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8326ad0>
2025-10-06 18:32:43.746 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8327f10>
2025-10-06 18:32:43.746 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8327e90>
2025-10-06 18:32:43.747 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca82f9490>
2025-10-06 18:32:43.748 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8327850>
2025-10-06 18:32:43.748 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8308e50>
2025-10-06 18:32:43.749 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833c210>
2025-10-06 18:32:43.750 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27cf9a10>
2025-10-06 18:32:43.750 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833c650>
2025-10-06 18:32:43.751 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8324710>
2025-10-06 18:32:43.751 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833c090>
2025-10-06 18:32:43.751 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833c990>
2025-10-06 18:32:43.751 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27cefd50>
2025-10-06 18:32:43.751 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833c910>
2025-10-06 18:32:43.751 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc805fe90>
2025-10-06 18:32:43.751 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8324550>
2025-10-06 18:32:43.751 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc805e550>
2025-10-06 18:32:43.753 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8324190>
2025-10-06 18:32:43.753 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27ce6c50>
2025-10-06 18:32:43.754 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833c9d0>
2025-10-06 18:32:43.755 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8308990>
2025-10-06 18:32:43.755 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca83259d0>
2025-10-06 18:32:43.756 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8324f10>
2025-10-06 18:32:43.756 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8327c50>
2025-10-06 18:32:43.757 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27cbd350>
2025-10-06 18:32:43.757 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833d190>
2025-10-06 18:32:43.757 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27ccbb90>
2025-10-06 18:32:43.757 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833ce10>
2025-10-06 18:32:43.757 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8319410>
2025-10-06 18:32:43.758 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833d290>
2025-10-06 18:32:43.758 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8324a10>
2025-10-06 18:32:43.758 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8324b90>
2025-10-06 18:32:43.758 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f37f419ff10>
2025-10-06 18:32:43.759 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27d02c90>
2025-10-06 18:32:43.760 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca82f9390>
2025-10-06 18:32:43.760 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8325750>
2025-10-06 18:32:43.761 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc80513d0>
2025-10-06 18:32:43.761 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc805c0d0>
2025-10-06 18:32:43.762 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8325410>
2025-10-06 18:32:43.762 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7fafcd0>
2025-10-06 18:32:43.762 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8325110>
2025-10-06 18:32:43.762 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8325610>
2025-10-06 18:32:43.762 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8035910>
2025-10-06 18:32:43.763 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8325b50>
2025-10-06 18:32:43.763 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27ceba10>
2025-10-06 18:32:43.763 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7fa9e90>
2025-10-06 18:32:43.764 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7fd93d0>
2025-10-06 18:32:43.765 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27cf3590>
2025-10-06 18:32:43.765 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833dd50>
2025-10-06 18:32:43.766 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833ddd0>
2025-10-06 18:32:43.766 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8309310>
2025-10-06 18:32:43.766 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833df90>
2025-10-06 18:32:43.768 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8326010>
2025-10-06 18:32:43.768 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8036510>
2025-10-06 18:32:43.769 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8036250>
2025-10-06 18:32:43.769 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833e390>
2025-10-06 18:32:43.770 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27e0e450>
2025-10-06 18:32:43.771 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833e290>
2025-10-06 18:32:43.771 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833de50>
2025-10-06 18:32:43.771 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8326450>
2025-10-06 18:32:43.771 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833e3d0>
2025-10-06 18:32:43.771 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8325150>
2025-10-06 18:32:43.771 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8326810>
2025-10-06 18:32:43.771 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27cef750>
2025-10-06 18:32:43.771 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833e990>
2025-10-06 18:32:43.772 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833ead0>
2025-10-06 18:32:43.773 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f37dc1180d0>
2025-10-06 18:32:43.774 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833eb10>
2025-10-06 18:32:43.774 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3c27d01410>
2025-10-06 18:32:43.775 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8036490>
2025-10-06 18:32:43.775 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833ee90>
2025-10-06 18:32:43.776 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833e910>
2025-10-06 18:32:43.777 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833ee10>
2025-10-06 18:32:43.777 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca8326f90>
2025-10-06 18:32:43.777 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8051250>
2025-10-06 18:32:43.777 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8036450>
2025-10-06 18:32:43.777 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833f250>
2025-10-06 18:32:43.777 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833f050>
2025-10-06 18:32:43.777 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca83269d0>
2025-10-06 18:32:43.777 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8037e90>
2025-10-06 18:32:43.777 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833f310>
2025-10-06 18:32:43.777 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833f510>
2025-10-06 18:32:43.778 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833f150>
2025-10-06 18:32:43.778 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833f190>
2025-10-06 18:32:43.778 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f37f4171110>
2025-10-06 18:32:43.778 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833f710>
2025-10-06 18:32:43.779 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7fa9110>
2025-10-06 18:32:43.779 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833f850>
2025-10-06 18:32:43.780 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833f650>
2025-10-06 18:32:43.780 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8053190>
2025-10-06 18:32:43.781 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca83011d0>
2025-10-06 18:32:43.782 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc8037a10>
2025-10-06 18:32:43.782 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca82f8b50>
2025-10-06 18:32:43.782 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833fc90>
2025-10-06 18:32:43.782 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3ca833fd90>
2025-10-06 18:32:43.782 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7f3bc7faff10>
2025-10-06 18:32:44.802 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=4, type='float', symmetric=True, group_size=None, strategy='tensor', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False
2025-10-06 18:32:46.209 | DEBUG    | llmcompressor.observers.base:record_observed_tokens:289 - The input tensor is expected to have two dimensions (batch_size * sequence_length, num_features). The input tensor has 3 dimensions.
