2025-10-06 18:11:54.226 | INFO     | llmcompressor.metrics.logger:_create_default_logger:357 - Logging all LLM Compressor modifier-level logs to sparse_logs/06-10-2025_18.11.54.log
2025-10-06 18:11:54.228 | DEBUG    | llmcompressor.core.lifecycle:initialize:92 - Initializing compression lifecycle
2025-10-06 18:11:54.229 | INFO     | llmcompressor.recipe.recipe:create_instance:140 - Loading recipe from file /gaueko1/users/mmartin/ptq_exp/yaml/int8_calib_recipe.yaml
2025-10-06 18:11:54.419 | DEBUG    | llmcompressor.core.lifecycle:initialize:105 - Initialized modifier: config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=False ended_=False
2025-10-06 18:11:54.419 | INFO     | llmcompressor.core.lifecycle:initialize:110 - Compression lifecycle initialized for 1 modifiers
2025-10-06 18:11:54.420 | INFO     | llmcompressor.pipelines.independent.pipeline:IndependentPipeline:43 - Inferred `SequentialPipeline` for `QuantizationModifier`
2025-10-06 18:12:01.074 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-06 18:12:01.076 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(input_ids, inputs_embeds):
    if (input_ids is None) ^ (inputs_embeds is not None):
        raise ValueError('You must specify exactly one of input_ids or inputs_embeds')
    return ()
2025-10-06 18:12:01.076 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-06 18:12:01.077 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-06 18:12:01.077 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_1(input_ids, inputs_embeds):
    if inputs_embeds is None:
        inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)
    return (inputs_embeds,)
2025-10-06 18:12:01.078 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-06 18:12:01.079 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-06 18:12:01.079 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_2(past_key_values, use_cache):
    if use_cache and past_key_values is None:
        past_key_values = DynamicCache(config=self.config)
    return (past_key_values,)
2025-10-06 18:12:01.080 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-06 18:12:01.080 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-06 18:12:01.081 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_3(cache_position, inputs_embeds, past_key_values, *, past_seen_tokens=None):
    if cache_position is None:
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        cache_position: torch.Tensor = torch.arange(past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device)
    return (cache_position, past_seen_tokens)
2025-10-06 18:12:01.081 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-06 18:12:01.082 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-06 18:12:01.082 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_4(cache_position, position_ids):
    if position_ids is None:
        position_ids = cache_position.unsqueeze(0)
    return (position_ids,)
2025-10-06 18:12:01.083 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-06 18:12:01.083 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-06 18:12:01.084 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_5(attention_mask, cache_position, inputs_embeds, past_key_values, position_ids):
    return create_causal_mask(config=self.config, input_embeds=inputs_embeds, attention_mask=attention_mask, cache_position=cache_position, past_key_values=past_key_values, position_ids=position_ids)
    return ()
2025-10-06 18:12:01.085 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-06 18:12:01.089 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2025-10-06 18:12:01.089 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(kwargs, labels, logits, loss):
    if labels is not None:
        loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)
    return (loss,)
2025-10-06 18:12:01.089 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2025-10-06 18:12:01.182 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.CALIBRATION_EPOCH_START
2025-10-06 18:12:01.183 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e36c10>
2025-10-06 18:12:01.183 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e35f50>
2025-10-06 18:12:01.184 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e103d0>
2025-10-06 18:12:01.185 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa2200f8650>
2025-10-06 18:12:01.185 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510efa10>
2025-10-06 18:12:01.186 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa22014fc10>
2025-10-06 18:12:01.186 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa19015f690>
2025-10-06 18:12:01.187 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa220169910>
2025-10-06 18:12:01.188 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510eef50>
2025-10-06 18:12:01.188 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e47fd0>
2025-10-06 18:12:01.189 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa1901db290>
2025-10-06 18:12:01.190 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510e9490>
2025-10-06 18:12:01.190 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e375d0>
2025-10-06 18:12:01.191 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e46bd0>
2025-10-06 18:12:01.191 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa22016b550>
2025-10-06 18:12:01.191 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e531d0>
2025-10-06 18:12:01.191 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa220168dd0>
2025-10-06 18:12:01.191 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e116d0>
2025-10-06 18:12:01.191 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa19015ee10>
2025-10-06 18:12:01.191 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa190151d50>
2025-10-06 18:12:01.193 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6081a8f50>
2025-10-06 18:12:01.193 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6081a9010>
2025-10-06 18:12:01.194 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e51810>
2025-10-06 18:12:01.194 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e44910>
2025-10-06 18:12:01.195 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e5fc50>
2025-10-06 18:12:01.196 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa19014fc90>
2025-10-06 18:12:01.196 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e46a10>
2025-10-06 18:12:01.197 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e5e990>
2025-10-06 18:12:01.197 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa190131850>
2025-10-06 18:12:01.197 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e5ff10>
2025-10-06 18:12:01.197 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa65106ab90>
2025-10-06 18:12:01.197 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa220168ed0>
2025-10-06 18:12:01.197 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e514d0>
2025-10-06 18:12:01.197 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e213d0>
2025-10-06 18:12:01.198 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e21550>
2025-10-06 18:12:01.199 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510eae90>
2025-10-06 18:12:01.199 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510fa350>
2025-10-06 18:12:01.200 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e70710>
2025-10-06 18:12:01.200 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e70890>
2025-10-06 18:12:01.201 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510ce890>
2025-10-06 18:12:01.202 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e70510>
2025-10-06 18:12:01.202 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa65110f290>
2025-10-06 18:12:01.203 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa19014ec10>
2025-10-06 18:12:01.203 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa19015e590>
2025-10-06 18:12:01.203 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e21250>
2025-10-06 18:12:01.203 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6081a9dd0>
2025-10-06 18:12:01.203 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e51a90>
2025-10-06 18:12:01.203 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e35210>
2025-10-06 18:12:01.203 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e45e90>
2025-10-06 18:12:01.204 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa22012aa10>
2025-10-06 18:12:01.205 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510ef6d0>
2025-10-06 18:12:01.205 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e51c90>
2025-10-06 18:12:01.205 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e71210>
2025-10-06 18:12:01.205 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e219d0>
2025-10-06 18:12:01.206 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e10090>
2025-10-06 18:12:01.206 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa22014e3d0>
2025-10-06 18:12:01.206 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510fab10>
2025-10-06 18:12:01.206 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510bec90>
2025-10-06 18:12:01.207 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510f0d90>
2025-10-06 18:12:01.208 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510b7710>
2025-10-06 18:12:01.208 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510ef710>
2025-10-06 18:12:01.209 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa190131010>
2025-10-06 18:12:01.209 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e71250>
2025-10-06 18:12:01.209 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e71490>
2025-10-06 18:12:01.209 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e235d0>
2025-10-06 18:12:01.209 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa1901536d0>
2025-10-06 18:12:01.209 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e5fdd0>
2025-10-06 18:12:01.209 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510b7650>
2025-10-06 18:12:01.211 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e51690>
2025-10-06 18:12:01.211 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510cbe50>
2025-10-06 18:12:01.212 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e11750>
2025-10-06 18:12:01.212 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e71c90>
2025-10-06 18:12:01.213 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e11790>
2025-10-06 18:12:01.214 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510b7690>
2025-10-06 18:12:01.214 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e13810>
2025-10-06 18:12:01.215 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa65110c050>
2025-10-06 18:12:01.216 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa19015fa90>
2025-10-06 18:12:01.216 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e50d10>
2025-10-06 18:12:01.217 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e52210>
2025-10-06 18:12:01.217 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa1901334d0>
2025-10-06 18:12:01.218 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e72010>
2025-10-06 18:12:01.218 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e5fe50>
2025-10-06 18:12:01.219 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510f2dd0>
2025-10-06 18:12:01.220 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510b6f50>
2025-10-06 18:12:01.220 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e47250>
2025-10-06 18:12:01.221 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e72690>
2025-10-06 18:12:01.221 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510eac50>
2025-10-06 18:12:01.221 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e725d0>
2025-10-06 18:12:01.221 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa1901db510>
2025-10-06 18:12:01.221 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e229d0>
2025-10-06 18:12:01.221 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510b7c10>
2025-10-06 18:12:01.222 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e72090>
2025-10-06 18:12:01.222 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510ceb90>
2025-10-06 18:12:01.223 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e53fd0>
2025-10-06 18:12:01.223 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e72b50>
2025-10-06 18:12:01.224 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e11d90>
2025-10-06 18:12:01.225 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510c90d0>
2025-10-06 18:12:01.225 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e5fed0>
2025-10-06 18:12:01.226 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa190153390>
2025-10-06 18:12:01.226 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e72990>
2025-10-06 18:12:01.227 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510e82d0>
2025-10-06 18:12:01.228 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa19015f010>
2025-10-06 18:12:01.228 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa190151b50>
2025-10-06 18:12:01.229 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510f8610>
2025-10-06 18:12:01.230 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa190133150>
2025-10-06 18:12:01.230 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e11510>
2025-10-06 18:12:01.230 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa19015ff10>
2025-10-06 18:12:01.230 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa190132010>
2025-10-06 18:12:01.230 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa65110efd0>
2025-10-06 18:12:01.230 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e461d0>
2025-10-06 18:12:01.230 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e222d0>
2025-10-06 18:12:01.231 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e72fd0>
2025-10-06 18:12:01.231 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510ee3d0>
2025-10-06 18:12:01.232 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e5ff50>
2025-10-06 18:12:01.233 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa19015c9d0>
2025-10-06 18:12:01.233 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e11f50>
2025-10-06 18:12:01.234 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e53650>
2025-10-06 18:12:01.234 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e73a50>
2025-10-06 18:12:01.235 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e13b10>
2025-10-06 18:12:01.236 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510b7ad0>
2025-10-06 18:12:01.237 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e523d0>
2025-10-06 18:12:01.237 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e5ff90>
2025-10-06 18:12:01.238 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510c6c10>
2025-10-06 18:12:01.239 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa190132b10>
2025-10-06 18:12:01.239 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6081b9990>
2025-10-06 18:12:01.240 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa19014e410>
2025-10-06 18:12:01.240 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa19014d2d0>
2025-10-06 18:12:01.241 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa190153ed0>
2025-10-06 18:12:01.242 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa190153d90>
2025-10-06 18:12:01.242 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510cad90>
2025-10-06 18:12:01.242 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e5ffd0>
2025-10-06 18:12:01.242 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e11550>
2025-10-06 18:12:01.242 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa65110ec10>
2025-10-06 18:12:01.242 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa19014c4d0>
2025-10-06 18:12:01.242 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e10410>
2025-10-06 18:12:01.242 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e106d0>
2025-10-06 18:12:01.243 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510b6250>
2025-10-06 18:12:01.244 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa190151b10>
2025-10-06 18:12:01.244 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa190132a90>
2025-10-06 18:12:01.244 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e35990>
2025-10-06 18:12:01.244 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e70690>
2025-10-06 18:12:01.245 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa190151110>
2025-10-06 18:12:01.245 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e51ed0>
2025-10-06 18:12:01.245 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e88350>
2025-10-06 18:12:01.245 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e23350>
2025-10-06 18:12:01.245 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e22ed0>
2025-10-06 18:12:01.246 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510fa7d0>
2025-10-06 18:12:01.246 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e443d0>
2025-10-06 18:12:01.247 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e21fd0>
2025-10-06 18:12:01.247 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e70450>
2025-10-06 18:12:01.247 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e71b10>
2025-10-06 18:12:01.247 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa65110e6d0>
2025-10-06 18:12:01.247 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa190152fd0>
2025-10-06 18:12:01.247 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e88e50>
2025-10-06 18:12:01.247 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510b4810>
2025-10-06 18:12:01.248 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e70490>
2025-10-06 18:12:01.249 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510eb850>
2025-10-06 18:12:01.250 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e71050>
2025-10-06 18:12:01.250 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e20950>
2025-10-06 18:12:01.251 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e210d0>
2025-10-06 18:12:01.251 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e11250>
2025-10-06 18:12:01.252 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e71450>
2025-10-06 18:12:01.253 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e89090>
2025-10-06 18:12:01.253 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e72350>
2025-10-06 18:12:01.254 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510c9f90>
2025-10-06 18:12:01.254 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa220128190>
2025-10-06 18:12:01.255 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e89290>
2025-10-06 18:12:01.255 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e89410>
2025-10-06 18:12:01.256 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e5c550>
2025-10-06 18:12:01.256 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e89810>
2025-10-06 18:12:01.257 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e89550>
2025-10-06 18:12:01.258 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa19015e3d0>
2025-10-06 18:12:01.258 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e89b10>
2025-10-06 18:12:01.258 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e89c50>
2025-10-06 18:12:01.258 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa190151e90>
2025-10-06 18:12:01.259 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e50a90>
2025-10-06 18:12:01.259 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e89b90>
2025-10-06 18:12:01.259 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e708d0>
2025-10-06 18:12:01.259 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e88f10>
2025-10-06 18:12:01.259 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510b72d0>
2025-10-06 18:12:01.260 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa190132090>
2025-10-06 18:12:01.260 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e72150>
2025-10-06 18:12:01.261 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e71590>
2025-10-06 18:12:01.262 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e89f50>
2025-10-06 18:12:01.262 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e11410>
2025-10-06 18:12:01.263 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8a410>
2025-10-06 18:12:01.263 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8a190>
2025-10-06 18:12:01.264 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa65110e450>
2025-10-06 18:12:01.265 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8a490>
2025-10-06 18:12:01.265 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e721d0>
2025-10-06 18:12:01.266 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e72610>
2025-10-06 18:12:01.266 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8a750>
2025-10-06 18:12:01.267 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8a890>
2025-10-06 18:12:01.267 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e73e50>
2025-10-06 18:12:01.267 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8a790>
2025-10-06 18:12:01.267 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8a290>
2025-10-06 18:12:01.267 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8a5d0>
2025-10-06 18:12:01.267 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8ac50>
2025-10-06 18:12:01.268 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e71f50>
2025-10-06 18:12:01.268 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa19014e750>
2025-10-06 18:12:01.268 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8aa50>
2025-10-06 18:12:01.268 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8add0>
2025-10-06 18:12:01.268 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6511056d0>
2025-10-06 18:12:01.268 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e71fd0>
2025-10-06 18:12:01.268 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e70610>
2025-10-06 18:12:01.268 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8ab90>
2025-10-06 18:12:01.268 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e89f90>
2025-10-06 18:12:01.269 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8ac10>
2025-10-06 18:12:01.270 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8ad90>
2025-10-06 18:12:01.271 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8b150>
2025-10-06 18:12:01.272 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8ae90>
2025-10-06 18:12:01.272 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6510b71d0>
2025-10-06 18:12:01.273 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e70650>
2025-10-06 18:12:01.274 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8ab10>
2025-10-06 18:12:01.274 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8b510>
2025-10-06 18:12:01.275 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8b250>
2025-10-06 18:12:01.275 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8b6d0>
2025-10-06 18:12:01.276 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e705d0>
2025-10-06 18:12:01.277 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8b790>
2025-10-06 18:12:01.277 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8aad0>
2025-10-06 18:12:01.278 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8b990>
2025-10-06 18:12:01.278 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e8ba50>
2025-10-06 18:12:01.279 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e71d10>
2025-10-06 18:12:01.280 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False added <torch.utils.hooks.RemovableHandle object at 0x7fa6d0e73b10>
2025-10-06 18:12:02.382 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups={'group_0': QuantizationScheme(targets=['Linear'], weights=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='channel', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), input_activations=QuantizationArgs(num_bits=8, type='int', symmetric=True, group_size=None, strategy='token', block_structure=None, dynamic=False, actorder=None, observer='minmax', observer_kwargs={}), output_activations=None, format=None)} targets=['Linear'] ignore=['lm_head'] scheme=None kv_cache_scheme=None index=None group='quant' start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False
2025-10-06 18:12:05.353 | DEBUG    | llmcompressor.observers.base:record_observed_tokens:289 - The input tensor is expected to have two dimensions (batch_size * sequence_length, num_features). The input tensor has 3 dimensions.
