wandb: Currently logged in as: maytemuma (maytemuma-upv-ehu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gaueko1/users/mmartin/tfm-quantization-llm/wandb/run-20251216_112819-wuzxsnah
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run latxa3.1_8b-lora-fp8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/maytemuma-upv-ehu/lora-fp8-latxa3.1_8b
wandb: üöÄ View run at https://wandb.ai/maytemuma-upv-ehu/lora-fp8-latxa3.1_8b/runs/wuzxsnah
`torch_dtype` is deprecated! Use `dtype` instead!
Compressing model: 0it [00:00, ?it/s]Compressing model: 43it [00:00, 419.34it/s]Compressing model: 113it [00:00, 571.89it/s]Compressing model: 175it [00:00, 590.39it/s]Compressing model: 224it [00:00, 560.41it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:01<00:01,  1.49s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.34s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.36s/it]
Map:   0%|          | 0/86722 [00:00<?, ? examples/s]Map:   0%|          | 371/86722 [00:00<00:31, 2700.75 examples/s]Map:   1%|          | 838/86722 [00:00<00:47, 1808.00 examples/s]Map:   2%|‚ñè         | 1369/86722 [00:00<00:41, 2050.76 examples/s]Map:   2%|‚ñè         | 2000/86722 [00:00<00:31, 2696.03 examples/s]Map:   3%|‚ñé         | 2766/86722 [00:00<00:22, 3791.96 examples/s]Map:   4%|‚ñç         | 3379/86722 [00:01<00:21, 3900.70 examples/s]Map:   5%|‚ñç         | 4000/86722 [00:01<00:20, 3968.40 examples/s]Map:   5%|‚ñå         | 4765/86722 [00:01<00:17, 4807.29 examples/s]Map:   6%|‚ñå         | 5370/86722 [00:01<00:17, 4521.40 examples/s]Map:   7%|‚ñã         | 6000/86722 [00:01<00:18, 4397.13 examples/s]Map:   8%|‚ñä         | 6754/86722 [00:01<00:15, 5121.07 examples/s]Map:   8%|‚ñä         | 7321/86722 [00:01<00:16, 4757.97 examples/s]Map:   9%|‚ñâ         | 8000/86722 [00:02<00:17, 4567.34 examples/s]Map:  10%|‚ñà         | 8677/86722 [00:02<00:15, 5078.16 examples/s]Map:  11%|‚ñà         | 9357/86722 [00:02<00:16, 4782.31 examples/s]Map:  12%|‚ñà‚ñè        | 10000/86722 [00:02<00:24, 3140.01 examples/s]Map:  12%|‚ñà‚ñè        | 10755/86722 [00:02<00:19, 3888.31 examples/s]Map:  13%|‚ñà‚ñé        | 11321/86722 [00:02<00:19, 3897.77 examples/s]Map:  14%|‚ñà‚ñç        | 12000/86722 [00:03<00:18, 4094.59 examples/s]Map:  15%|‚ñà‚ñç        | 12706/86722 [00:03<00:15, 4717.05 examples/s]Map:  15%|‚ñà‚ñå        | 13328/86722 [00:03<00:16, 4535.47 examples/s]Map:  16%|‚ñà‚ñå        | 14000/86722 [00:03<00:15, 4552.09 examples/s]Map:  17%|‚ñà‚ñã        | 14709/86722 [00:03<00:14, 5129.04 examples/s]Map:  18%|‚ñà‚ñä        | 15330/86722 [00:03<00:15, 4572.37 examples/s]Map:  18%|‚ñà‚ñä        | 16000/86722 [00:03<00:15, 4560.89 examples/s]Map:  19%|‚ñà‚ñâ        | 16711/86722 [00:03<00:13, 5141.06 examples/s]Map:  20%|‚ñà‚ñâ        | 17295/86722 [00:04<00:14, 4743.40 examples/s]Map:  21%|‚ñà‚ñà        | 18000/86722 [00:04<00:14, 4689.69 examples/s]Map:  22%|‚ñà‚ñà‚ñè       | 18711/86722 [00:04<00:12, 5248.56 examples/s]Map:  22%|‚ñà‚ñà‚ñè       | 19333/86722 [00:04<00:21, 3167.29 examples/s]Map:  23%|‚ñà‚ñà‚ñé       | 20000/86722 [00:04<00:19, 3485.23 examples/s]Map:  24%|‚ñà‚ñà‚ñç       | 20699/86722 [00:05<00:15, 4128.69 examples/s]Map:  25%|‚ñà‚ñà‚ñç       | 21331/86722 [00:05<00:15, 4158.27 examples/s]Map:  25%|‚ñà‚ñà‚ñå       | 22000/86722 [00:05<00:15, 4281.82 examples/s]Map:  26%|‚ñà‚ñà‚ñå       | 22711/86722 [00:05<00:13, 4893.57 examples/s]Map:  27%|‚ñà‚ñà‚ñã       | 23328/86722 [00:05<00:13, 4669.11 examples/s]Map:  28%|‚ñà‚ñà‚ñä       | 24000/86722 [00:05<00:13, 4622.97 examples/s]Map:  28%|‚ñà‚ñà‚ñä       | 24710/86722 [00:05<00:11, 5191.02 examples/s]Map:  29%|‚ñà‚ñà‚ñâ       | 25329/86722 [00:05<00:12, 4860.15 examples/s]Map:  30%|‚ñà‚ñà‚ñâ       | 26000/86722 [00:06<00:12, 4785.30 examples/s]Map:  31%|‚ñà‚ñà‚ñà       | 26722/86722 [00:06<00:11, 5359.21 examples/s]Map:  32%|‚ñà‚ñà‚ñà‚ñè      | 27334/86722 [00:06<00:11, 4984.58 examples/s]Map:  32%|‚ñà‚ñà‚ñà‚ñè      | 28000/86722 [00:06<00:18, 3191.85 examples/s]Map:  33%|‚ñà‚ñà‚ñà‚ñé      | 28674/86722 [00:06<00:15, 3798.17 examples/s]Map:  34%|‚ñà‚ñà‚ñà‚ñç      | 29328/86722 [00:06<00:14, 3930.20 examples/s]Map:  35%|‚ñà‚ñà‚ñà‚ñç      | 30000/86722 [00:07<00:15, 3777.19 examples/s]Map:  35%|‚ñà‚ñà‚ñà‚ñå      | 30712/86722 [00:07<00:12, 4430.00 examples/s]Map:  36%|‚ñà‚ñà‚ñà‚ñå      | 31322/86722 [00:07<00:12, 4343.71 examples/s]Map:  37%|‚ñà‚ñà‚ñà‚ñã      | 32000/86722 [00:07<00:12, 4413.03 examples/s]Map:  38%|‚ñà‚ñà‚ñà‚ñä      | 32703/86722 [00:07<00:10, 4991.54 examples/s]Map:  38%|‚ñà‚ñà‚ñà‚ñä      | 33324/86722 [00:07<00:11, 4719.49 examples/s]Map:  39%|‚ñà‚ñà‚ñà‚ñâ      | 34000/86722 [00:07<00:11, 4691.31 examples/s]Map:  40%|‚ñà‚ñà‚ñà‚ñà      | 34704/86722 [00:08<00:09, 5236.30 examples/s]Map:  41%|‚ñà‚ñà‚ñà‚ñà      | 35328/86722 [00:08<00:10, 4890.52 examples/s]Map:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 36000/86722 [00:08<00:10, 4799.53 examples/s]Map:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 36719/86722 [00:08<00:09, 5363.49 examples/s]Map:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 37581/86722 [00:08<00:14, 3470.80 examples/s]Map:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 38331/86722 [00:09<00:12, 3746.10 examples/s]Map:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 39000/86722 [00:09<00:12, 3940.19 examples/s]Map:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 39713/86722 [00:09<00:10, 4546.33 examples/s]Map:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 40325/86722 [00:09<00:10, 4453.83 examples/s]Map:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 41000/86722 [00:09<00:10, 4486.01 examples/s]Map:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 41710/86722 [00:09<00:08, 5059.63 examples/s]Map:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 42327/86722 [00:09<00:09, 4794.81 examples/s]Map:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 43000/86722 [00:09<00:09, 4706.85 examples/s]Map:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 43716/86722 [00:10<00:08, 5272.69 examples/s]Map:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 44305/86722 [00:10<00:08, 4869.21 examples/s]Map:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 45000/86722 [00:10<00:08, 4775.22 examples/s]Map:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 45712/86722 [00:10<00:07, 5323.66 examples/s]Map:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 46322/86722 [00:10<00:08, 4933.34 examples/s]Map:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 47000/86722 [00:10<00:12, 3269.98 examples/s]Map:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 47706/86722 [00:11<00:09, 3928.78 examples/s]Map:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 48327/86722 [00:11<00:09, 3987.05 examples/s]Map:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 49000/86722 [00:11<00:09, 4156.90 examples/s]Map:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 49708/86722 [00:11<00:07, 4774.35 examples/s]Map:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 50312/86722 [00:11<00:08, 4537.15 examples/s]Map:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 51000/86722 [00:11<00:07, 4554.30 examples/s]Map:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 51714/86722 [00:11<00:06, 5138.75 examples/s]Map:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 52332/86722 [00:12<00:07, 4809.79 examples/s]Map:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 53000/86722 [00:12<00:07, 4740.95 examples/s]Map:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 53709/86722 [00:12<00:06, 5289.30 examples/s]Map:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 54324/86722 [00:12<00:06, 4894.36 examples/s]Map:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 55000/86722 [00:12<00:06, 4802.14 examples/s]Map:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 55585/86722 [00:12<00:09, 3413.54 examples/s]Map:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 56000/86722 [00:12<00:08, 3455.89 examples/s]Map:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 56716/86722 [00:13<00:07, 4215.97 examples/s]Map:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 57323/86722 [00:13<00:07, 4191.24 examples/s]Map:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 58000/86722 [00:13<00:06, 4299.27 examples/s]Map:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 58719/86722 [00:13<00:05, 4949.80 examples/s]Map:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 59329/86722 [00:13<00:07, 3749.40 examples/s]Map:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 60000/86722 [00:13<00:06, 3972.34 examples/s]Map:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 60719/86722 [00:13<00:05, 4636.81 examples/s]Map:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 61330/86722 [00:14<00:05, 4497.58 examples/s]Map:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 62000/86722 [00:14<00:05, 4530.99 examples/s]Map:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 62714/86722 [00:14<00:04, 5123.54 examples/s]Map:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 63324/86722 [00:14<00:04, 4820.26 examples/s]Map:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 64000/86722 [00:14<00:04, 4744.82 examples/s]Map:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 64715/86722 [00:14<00:04, 5308.84 examples/s]Map:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 65301/86722 [00:15<00:06, 3191.86 examples/s]Map:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 66000/86722 [00:15<00:05, 3488.25 examples/s]Map:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 66666/86722 [00:15<00:04, 4069.33 examples/s]Map:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 67312/86722 [00:15<00:04, 4058.15 examples/s]Map:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 68000/86722 [00:15<00:04, 4154.50 examples/s]Map:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 68667/86722 [00:15<00:03, 4682.48 examples/s]Map:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 69308/86722 [00:15<00:03, 4484.33 examples/s]Map:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 70000/86722 [00:16<00:03, 4448.37 examples/s]Map:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 70668/86722 [00:16<00:03, 4942.10 examples/s]Map:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 71308/86722 [00:16<00:03, 4658.84 examples/s]Map:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 72000/86722 [00:16<00:03, 4587.42 examples/s]Map:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 72677/86722 [00:16<00:02, 5080.47 examples/s]Map:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 73313/86722 [00:16<00:02, 4747.99 examples/s]Map:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 73981/86722 [00:17<00:03, 3310.65 examples/s]Map:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 74674/86722 [00:17<00:03, 3541.59 examples/s]Map:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 75301/86722 [00:17<00:03, 3670.34 examples/s]Map:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 76000/86722 [00:17<00:02, 3900.29 examples/s]Map:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 76692/86722 [00:17<00:02, 4501.35 examples/s]Map:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 77316/86722 [00:17<00:02, 4395.78 examples/s]Map:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 78000/86722 [00:18<00:01, 4437.15 examples/s]Map:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 78689/86722 [00:18<00:01, 4978.79 examples/s]Map:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 79311/86722 [00:18<00:01, 4714.83 examples/s]Map:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 80000/86722 [00:18<00:01, 4698.08 examples/s]Map:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 80675/86722 [00:18<00:01, 5167.71 examples/s]Map:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 81296/86722 [00:18<00:01, 4750.21 examples/s]Map:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 82000/86722 [00:18<00:01, 4651.40 examples/s]Map:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 82683/86722 [00:18<00:00, 5149.48 examples/s]Map:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 83336/86722 [00:19<00:00, 4935.94 examples/s]Map:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 83941/86722 [00:19<00:00, 3568.68 examples/s]Map:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 84745/86722 [00:19<00:00, 3951.99 examples/s]Map:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 85342/86722 [00:19<00:00, 4074.17 examples/s]Map:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 86000/86722 [00:19<00:00, 4300.90 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86722/86722 [00:19<00:00, 4607.08 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86722/86722 [00:19<00:00, 4338.70 examples/s]
Map:   0%|          | 0/10840 [00:00<?, ? examples/s]Map:   7%|‚ñã         | 782/10840 [00:00<00:01, 7682.25 examples/s]Map:  16%|‚ñà‚ñå        | 1739/10840 [00:00<00:01, 5944.82 examples/s]Map:  25%|‚ñà‚ñà‚ñå       | 2750/10840 [00:00<00:01, 5697.05 examples/s]Map:  31%|‚ñà‚ñà‚ñà       | 3343/10840 [00:00<00:01, 5247.46 examples/s]Map:  37%|‚ñà‚ñà‚ñà‚ñã      | 4000/10840 [00:00<00:01, 5106.21 examples/s]Map:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 4752/10840 [00:00<00:01, 5729.59 examples/s]Map:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 5346/10840 [00:00<00:01, 5318.93 examples/s]Map:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 5981/10840 [00:01<00:01, 3601.47 examples/s]Map:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 6720/10840 [00:01<00:01, 3895.35 examples/s]Map:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 7361/10840 [00:01<00:00, 3982.97 examples/s]Map:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 8000/10840 [00:01<00:00, 4094.20 examples/s]Map:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8687/10840 [00:01<00:00, 4679.16 examples/s]Map:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 9311/10840 [00:01<00:00, 4519.81 examples/s]Map:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 10000/10840 [00:02<00:00, 4543.99 examples/s]Map:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 10678/10840 [00:02<00:00, 5050.15 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10840/10840 [00:02<00:00, 4669.53 examples/s]
/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, dataset_num_proc, max_seq_length. Will not be supported from version '1.0.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:297: UserWarning: You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
Map (num_proc=4):   0%|          | 0/86722 [00:00<?, ? examples/s]Map (num_proc=4):   1%|          | 1000/86722 [00:01<02:43, 525.67 examples/s]Map (num_proc=4):   2%|‚ñè         | 2000/86722 [00:02<01:24, 998.29 examples/s]Map (num_proc=4):   5%|‚ñç         | 4000/86722 [00:02<00:41, 1990.49 examples/s]Map (num_proc=4):   8%|‚ñä         | 7000/86722 [00:03<00:23, 3330.75 examples/s]Map (num_proc=4):  12%|‚ñà‚ñè        | 10000/86722 [00:03<00:14, 5158.30 examples/s]Map (num_proc=4):  13%|‚ñà‚ñé        | 11000/86722 [00:03<00:15, 4984.10 examples/s]Map (num_proc=4):  16%|‚ñà‚ñå        | 14000/86722 [00:03<00:10, 6968.63 examples/s]Map (num_proc=4):  17%|‚ñà‚ñã        | 15000/86722 [00:03<00:11, 6506.78 examples/s]Map (num_proc=4):  20%|‚ñà‚ñâ        | 17000/86722 [00:04<00:08, 8082.38 examples/s]Map (num_proc=4):  22%|‚ñà‚ñà‚ñè       | 19000/86722 [00:04<00:09, 7482.68 examples/s]Map (num_proc=4):  25%|‚ñà‚ñà‚ñå       | 22000/86722 [00:04<00:07, 8847.10 examples/s]Map (num_proc=4):  28%|‚ñà‚ñà‚ñä       | 24000/86722 [00:04<00:07, 8955.68 examples/s]Map (num_proc=4):  30%|‚ñà‚ñà‚ñâ       | 26000/86722 [00:05<00:06, 8973.58 examples/s]Map (num_proc=4):  31%|‚ñà‚ñà‚ñà       | 27000/86722 [00:05<00:07, 8449.87 examples/s]Map (num_proc=4):  35%|‚ñà‚ñà‚ñà‚ñç      | 30000/86722 [00:05<00:06, 9421.78 examples/s]Map (num_proc=4):  36%|‚ñà‚ñà‚ñà‚ñå      | 31000/86722 [00:05<00:06, 8982.15 examples/s]Map (num_proc=4):  39%|‚ñà‚ñà‚ñà‚ñâ      | 34000/86722 [00:05<00:05, 9679.43 examples/s]Map (num_proc=4):  40%|‚ñà‚ñà‚ñà‚ñà      | 35000/86722 [00:06<00:05, 8972.16 examples/s]Map (num_proc=4):  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 38000/86722 [00:06<00:05, 9563.11 examples/s]Map (num_proc=4):  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 39000/86722 [00:06<00:05, 9282.39 examples/s]Map (num_proc=4):  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 41000/86722 [00:06<00:04, 10892.24 examples/s]Map (num_proc=4):  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 43000/86722 [00:06<00:05, 8353.73 examples/s] Map (num_proc=4):  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 46000/86722 [00:07<00:04, 9842.78 examples/s]Map (num_proc=4):  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 48000/86722 [00:07<00:04, 8978.12 examples/s]Map (num_proc=4):  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 50000/86722 [00:07<00:03, 9821.43 examples/s]Map (num_proc=4):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 52000/86722 [00:07<00:03, 9302.67 examples/s]Map (num_proc=4):  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 54000/86722 [00:08<00:03, 9531.81 examples/s]Map (num_proc=4):  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 55000/86722 [00:08<00:03, 8220.27 examples/s]Map (num_proc=4):  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 57000/86722 [00:08<00:03, 9758.41 examples/s]Map (num_proc=4):  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 59000/86722 [00:08<00:03, 8358.93 examples/s]Map (num_proc=4):  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 61000/86722 [00:08<00:02, 9902.82 examples/s]Map (num_proc=4):  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 63000/86722 [00:09<00:02, 8520.00 examples/s]Map (num_proc=4):  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 65000/86722 [00:09<00:02, 9814.30 examples/s]Map (num_proc=4):  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 67000/86722 [00:09<00:02, 8619.48 examples/s]Map (num_proc=4):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 69000/86722 [00:09<00:01, 9300.44 examples/s]Map (num_proc=4):  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 71000/86722 [00:09<00:01, 8663.51 examples/s]Map (num_proc=4):  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 72000/86722 [00:10<00:01, 7426.66 examples/s]Map (num_proc=4):  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 74000/86722 [00:10<00:01, 8936.88 examples/s]Map (num_proc=4):  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 76000/86722 [00:10<00:01, 7948.06 examples/s]Map (num_proc=4):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 77681/86722 [00:10<00:00, 9321.60 examples/s]Map (num_proc=4):  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 79681/86722 [00:11<00:01, 6971.62 examples/s]Map (num_proc=4):  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 81681/86722 [00:11<00:00, 6902.32 examples/s]Map (num_proc=4):  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 84362/86722 [00:12<00:00, 6180.03 examples/s]Map (num_proc=4):  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 86042/86722 [00:12<00:00, 6481.10 examples/s]Map (num_proc=4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86722/86722 [00:13<00:00, 6645.95 examples/s]
Map (num_proc=4):   0%|          | 0/10840 [00:00<?, ? examples/s]Map (num_proc=4):   9%|‚ñâ         | 1000/10840 [00:01<00:14, 694.37 examples/s]Map (num_proc=4):  18%|‚ñà‚ñä        | 2000/10840 [00:01<00:06, 1424.46 examples/s]Map (num_proc=4):  28%|‚ñà‚ñà‚ñä       | 3000/10840 [00:01<00:03, 2086.79 examples/s]Map (num_proc=4):  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 5000/10840 [00:02<00:01, 3812.19 examples/s]Map (num_proc=4):  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 6710/10840 [00:02<00:00, 5417.40 examples/s]Map (num_proc=4):  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 8420/10840 [00:02<00:00, 6078.22 examples/s]Map (num_proc=4):  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 9420/10840 [00:02<00:00, 6506.18 examples/s]Map (num_proc=4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10840/10840 [00:02<00:00, 5569.37 examples/s]Map (num_proc=4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10840/10840 [00:03<00:00, 3600.85 examples/s]
/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:412: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Traceback (most recent call last):
  File "/gaueko1/users/mmartin/tfm-quantization-llm/src/LoRa/scripts/train.py", line 237, in <module>
    outputs = model(
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/peft/peft_model.py", line 1577, in forward
    return self.base_model(
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/peft/tuners/tuners_utils.py", line 188, in forward
    return self.model.forward(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/transformers/utils/generic.py", line 940, in wrapper
    output = func(self, *args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 459, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/transformers/utils/generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 395, in forward
    hidden_states = decoder_layer(
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/transformers/modeling_layers.py", line 93, in __call__
    return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/_compile.py", line 53, in inner
    return disable_fn(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 929, in _fn
    return fn(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/utils/checkpoint.py", line 488, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/autograd/function.py", line 576, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/utils/checkpoint.py", line 262, in forward
    outputs = run_function(*args)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 294, in forward
    hidden_states, _ = self.self_attn(
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 236, in forward
    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/peft/tuners/lora/layer.py", line 556, in forward
    result = result + lora_B(lora_A(dropout(x))) * scaling
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/nn/modules/dropout.py", line 70, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/gaueko1/users/mmartin/qloraTrain/lib/python3.9/site-packages/torch/nn/functional.py", line 1422, in dropout
    _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
NotImplementedError: "fused_dropout" not implemented for 'Float8_e4m3fn'
