========================================
Inference VRAM/latency measurement
Job ID: 13570
Node: localhost
Started: Tue Dec 16 12:47:09 CET 2025
========================================

Environment Check:
Python 3.9.7
compressed-tensors       0.12.1
peft                     0.12.0
torch                    2.5.1
transformers             4.50.3
trl                      0.10.1

GPU Information (initial):
Tue Dec 16 12:47:12 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:48:00.0 Off |                    0 |
| N/A   36C    P0             35W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:89:00.0 Off |                    0 |
| N/A   34C    P0             37W /  250W |       0MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:C1:00.0 Off |                    0 |
| N/A   36C    P0             37W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Running inference measurement...
>>> Antes del patch, _init_weights = <function LlamaPreTrainedModel._init_weights at 0x7f86fef28940>
>>> Despu√©s del patch, _init_weights = <function patched_init_weights at 0x7f884da22550>
================================================================================
MODEL: /gaueko1/users/mmartin/ENVIRONMENT/models/Latxa_8b_loraMIO_quantized8bit
PROMPT (truncated): Kaixo, azaldu zer da LoRA euskaraz laburki....
max_new_tokens=128, batch_size=1
================================================================================
[nvidia-smi][before_infer] 2025/12/16 12:48:42.876, 0, Tesla V100-PCIE-32GB, 32768, 9156, 23339, 3
2025/12/16 12:48:42.877, 1, Tesla V100-PCIE-32GB, 32768, 0, 32495, 0
2025/12/16 12:48:42.877, 2, Tesla V100-PCIE-32GB, 32768, 0, 32495, 0
[nvidia-smi][after_infer] 2025/12/16 12:49:06.298, 0, Tesla V100-PCIE-32GB, 32768, 9316, 23179, 34
2025/12/16 12:49:06.299, 1, Tesla V100-PCIE-32GB, 32768, 0, 32495, 0
2025/12/16 12:49:06.299, 2, Tesla V100-PCIE-32GB, 32768, 0, 32495, 0
[METRICS] inference_time_sec=23.3231
[METRICS] peak_vram_gb=8.6078
================================================================================

========================================
Finished with exit code: 0
Finished: Tue Dec 16 12:49:08 CET 2025
========================================
